{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jJuTcO7WOc07"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chainer\n",
    "\n",
    "from chainer import cuda,report,training,utils,Variable\n",
    "from chainer import datasets,iterators,optimizers,serializers\n",
    "from chainer import Link,Chain,ChainList\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.training import extensions\n",
    "from chainer.datasets import tuple_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from hyperopt import fmin, tpe, hp\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6106,
     "status": "ok",
     "timestamp": 1545265275780,
     "user": {
      "displayName": "斉藤慎太郎",
      "photoUrl": "",
      "userId": "08781351261207481557"
     },
     "user_tz": -540
    },
    "id": "gJQO0yaDObE3",
    "outputId": "870cbbe0-7549-427c-b9f4-52516c500637"
   },
   "outputs": [],
   "source": [
    "##GPU環境の設定を確認\n",
    "print('GPU availability:', chainer.cuda.available)\n",
    "print('cuDNN availablility:', chainer.cuda.cudnn_enabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bKainsNJSpHT"
   },
   "outputs": [],
   "source": [
    "DATA_PATH='./'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HxtTlj3pObE8"
   },
   "outputs": [],
   "source": [
    "##データの読み込み\n",
    "import pickle\n",
    "\n",
    "## train data\n",
    "f=open(DATA_PATH+\"/x_train_corr_high.txt\",\"rb\")\n",
    "pre_data_train=pickle.load(f)\n",
    "f=open(DATA_PATH+\"/y_train_corr_high.txt\",\"rb\")\n",
    "label_train=pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 不均衡データなので、\n",
    "正解データと不例データから均等にサンプリングして複数データセットをひとつにまとめたものをデータセットにする<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "validationデータを正例、不例の一定割合データセットに分ける\n",
    "正例、不例のindexを取得しておく\n",
    "\n",
    "train時にはバッチサイズ64として、正例と不例を32ずつindexをサンプリングしたデータセットを複数作成して、まとめてデータセットとする\n",
    "正例、不例一定割合のデータセット作成\n",
    "'''\n",
    "\n",
    "buy=[]\n",
    "no_buy=[]\n",
    "for i in range(len(label_train)):\n",
    "    if label_train[i]==1:\n",
    "        buy.append(i)\n",
    "    else:\n",
    "        no_buy.append(i)\n",
    "        \n",
    "        \n",
    "x=[]\n",
    "y=[]\n",
    "\n",
    "'''make train data'''\n",
    "for i in range(2000):\n",
    "    posi=np.random.choice(buy,32,replace = False)\n",
    "    nega=np.random.choice(no_buy,32,replace = False)\n",
    "    for i in posi:\n",
    "        x.append(pre_data_train[i])\n",
    "        y.append(label_train[i])\n",
    "    for i in nega:\n",
    "        x.append(pre_data_train[i])\n",
    "        y.append(label_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**hyperoptで調べたいこと<br>\n",
    "・convolutionの数、チャネルの数、filaの数、padの数<br>\n",
    "・最適化関数の設定**<br>\n",
    "※フィルタサイズとパディングは固定して、いくつか試して良い物を選択する。仮に複数フィルタサイズを試したいなら、for文を積むように書いて行う\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ネットワーク定義\n",
    "\n",
    "\n",
    "class Purchase(Chain):\n",
    "    def __init__(self,out_chanel,mid_units,conv_num,activate):\n",
    "        super(Purchase , self).__init__(\n",
    "            ##k_size:5\n",
    "            conv1=L.Convolution2D(1,out_chanel,ksize=5,pad=2),\n",
    "            conv2=L.Convolution2D(out_chanel,out_chanel,ksize=5,pad=2),\n",
    "            ##k_size:3\n",
    "            conv3=L.Convolution2D(out_chanel,out_chanel,ksize=3,pad=1),\n",
    "            conv4=L.Convolution2D(out_chanel,out_chanel,ksize=3,pad=1),\n",
    "            conv5=L.Convolution2D(out_chanel,out_chanel,ksize=3,pad=1),\n",
    "            bn1=L.BatchNormalization(out_chanel),\n",
    "            bn2=L.BatchNormalization(out_chanel),\n",
    "            bn3=L.BatchNormalization(out_chanel),\n",
    "            bn4=L.BatchNormalization(out_chanel),\n",
    "            bn5=L.BatchNormalization(out_chanel),\n",
    "            l1=L.Linear(None,mid_units),\n",
    "            l2=L.Linear(mid_units,2)\n",
    "           \n",
    "        \n",
    "        )\n",
    "        self.conv_num = conv_num\n",
    "        if activate == 'relu':\n",
    "            self.act = F.relu\n",
    "        elif activate == 'tanh':\n",
    "            self.act = F.tanh\n",
    "        elif activate == 'sigmoid':\n",
    "            self.act = F.sigmoid\n",
    "        \n",
    "    def __call__(self,x):\n",
    "            \n",
    "            h1=self.act(self.bn1(self.conv1(x)))\n",
    "            h2=self.act(self.bn2(self.conv2(h1)))\n",
    "            if self.conv_num == 2:\n",
    "                h=F.dropout(self.act(self.l1(h2)))\n",
    "                return self.l2(h)\n",
    "            h3=self.act(self.bn3(self.conv3(h2)))\n",
    "            if self.conv_num == 3:\n",
    "                h=F.dropout(self.act(self.l1(h3)))\n",
    "                return self.l2(h)\n",
    "            h4=self.act(self.bn4(self.conv4(h3)))\n",
    "            if self.conv_num == 4:\n",
    "                h=F.dropout(self.act(self.l1(h4)))\n",
    "                return self.l2(h)\n",
    "            h5=self.act(self.bn5(self.conv5(h4)))\n",
    "            if self.conv_num == 5:\n",
    "                h=F.dropout(self.act(self.l1(h5)))\n",
    "                return self.l2(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(params):\n",
    "    epoch = 20\n",
    "    gpu = 0\n",
    "    batchsize = 64\n",
    "\n",
    "    out_chanel = params['out_chanel']\n",
    "    mid_units = params['mid_units']\n",
    "    conv_num = params['conv_num']\n",
    "    activate = params['activate']\n",
    "    optimizer_name = params['optimizer_name']\n",
    "    lr = params['lr']\n",
    "    weight=params['weight']\n",
    "\n",
    "    model = L.Classifier(Purchase(out_chanel, mid_units, conv_num, activate))\n",
    "    \n",
    "    if gpu >= 0:\n",
    "        chainer.cuda.get_device(gpu).use()\n",
    "        model.to_gpu()\n",
    "\n",
    "    # Setup an optimizer\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = optimizers.Adam()\n",
    "    elif optimizer_name == 'AdaDelta':\n",
    "        optimizer = optimizers.AdaDelta()\n",
    "    else:\n",
    "        optimizer = optimizers.MomentumSGD(lr=lr)\n",
    "    optimizer.setup(model)\n",
    "    optimizer.add_hook(chainer.optimizer_hooks.WeightDecay(weight))\n",
    "    \n",
    "    ##load data\n",
    "    ##x,yのデータをtuple化して、更にvalidation dataを作成する\n",
    "\n",
    "    data= tuple_dataset.TupleDataset(x,y)\n",
    "    split_at=int(len(x)*0.95)\n",
    "    train,test=chainer.datasets.split_dataset(data ,split_at)\n",
    "    \n",
    "    ##trainer\n",
    "    train_iter = chainer.iterators.SerialIterator(train, batchsize)\n",
    "    test_iter = chainer.iterators.SerialIterator(test, batchsize,repeat=False, shuffle=False)\n",
    "\n",
    "    # Set up a trainer\n",
    "    updater = training.StandardUpdater(train_iter, optimizer, device=gpu)\n",
    "    trainer = training.Trainer(updater, (epoch, 'epoch'), out=\"hyperopt_result\")\n",
    "    \n",
    "    # Evaluate the model with the test dataset for each epoch\n",
    "    trainer.extend(extensions.Evaluator(test_iter, model, device=gpu))\n",
    "\n",
    "    # Write a log of evaluation statistics for each epoch\n",
    "    trainer.extend(extensions.LogReport())\n",
    "\n",
    "    # Save two plot images to the result dir\n",
    "    trainer.extend(\n",
    "        extensions.PlotReport(['main/loss', 'validation/main/loss'],\n",
    "                               'epoch', file_name='loss.png'))\n",
    "    trainer.extend(\n",
    "        extensions.PlotReport(\n",
    "            ['main/accuracy', 'validation/main/accuracy'],\n",
    "             'epoch', file_name='accuracy.png'))\n",
    "\n",
    "    # Write report\n",
    "    trainer.extend(extensions.PrintReport(\n",
    "        ['epoch', 'main/loss', 'validation/main/loss',\n",
    "         'main/accuracy', 'validation/main/accuracy', 'elapsed_time']))\n",
    "\n",
    "    # Print a progress bar to stdout\n",
    "    trainer.extend(extensions.ProgressBar())\n",
    "\n",
    "    # Run the training\n",
    "    trainer.run()\n",
    "    valid_data = trainer._extensions['PlotReport'].extension._data\n",
    "    loss_data = [data for i, data in valid_data['validation/main/loss']]\n",
    "    best10_loss = sorted(loss_data)[:10]\n",
    "    return sum(best10_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    ##探索空間の設定\n",
    "    space = {\n",
    "        'out_chanel':scope.int(hp.quniform('out_chanel',100,300,50)),\n",
    "        'mid_units':scope.int(hp.quniform('mid_units',50,300,50)),\n",
    "        'conv_num':scope.int(hp.quniform('conv_num',2,5,1)),\n",
    "        'activate':hp.choice('activate',('relu','tanh','sigmoid')),\n",
    "        'optimizer_name':hp.choice('optimizer_name',('Adam','AdaDelta','MomentumSGD')),\n",
    "        'lr':hp.uniform('lr',0.005,0.02),\n",
    "        'weight':hp.uniform('weight',0.0001,0.005),\n",
    "        \n",
    "    }\n",
    "    best = fmin(main, space, algo=tpe.suggest, max_evals=100)\n",
    "    print(\"best parameters\",best)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "purchase_predcit2_data_modify.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
