{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5023,
     "status": "ok",
     "timestamp": 1547809992024,
     "user": {
      "displayName": "斉藤慎太郎",
      "photoUrl": "",
      "userId": "08781351261207481557"
     },
     "user_tz": -540
    },
    "id": "c7m21hRQObE1",
    "outputId": "e15a2c88-88e6-4695-d622-a9c5e72f10ac"
   },
   "outputs": [],
   "source": [
    "##google colab用にchainerのインストール\n",
    "!curl https://colab.chainer.org/install | sh -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 683,
     "status": "ok",
     "timestamp": 1547809992025,
     "user": {
      "displayName": "斉藤慎太郎",
      "photoUrl": "",
      "userId": "08781351261207481557"
     },
     "user_tz": -540
    },
    "id": "G4LcZg1ARPlt",
    "outputId": "39931736-101c-4b67-c515-deddcc55d06e"
   },
   "outputs": [],
   "source": [
    "##google driveのマウント\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3957,
     "status": "ok",
     "timestamp": 1547809997965,
     "user": {
      "displayName": "斉藤慎太郎",
      "photoUrl": "",
      "userId": "08781351261207481557"
     },
     "user_tz": -540
    },
    "id": "mVIJuqSq4YDR",
    "outputId": "ea50227f-6844-473c-b4fa-fed14ae37131"
   },
   "outputs": [],
   "source": [
    "##pip installする\n",
    "!pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jJuTcO7WOc07"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chainer\n",
    "\n",
    "from chainer import cuda,report,training,utils,Variable\n",
    "from chainer import datasets,iterators,optimizers,serializers\n",
    "from chainer import Link,Chain,ChainList\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.training import extensions\n",
    "from chainer.datasets import tuple_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from hyperopt import fmin, tpe, hp\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2343,
     "status": "ok",
     "timestamp": 1547809998182,
     "user": {
      "displayName": "斉藤慎太郎",
      "photoUrl": "",
      "userId": "08781351261207481557"
     },
     "user_tz": -540
    },
    "id": "gJQO0yaDObE3",
    "outputId": "a843eba1-c1ed-4214-d123-a0ccab88311c"
   },
   "outputs": [],
   "source": [
    "##GPU環境の設定を確認\n",
    "print('GPU availability:', chainer.cuda.available)\n",
    "print('cuDNN availablility:', chainer.cuda.cudnn_enabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3581,
     "status": "ok",
     "timestamp": 1547809999777,
     "user": {
      "displayName": "斉藤慎太郎",
      "photoUrl": "",
      "userId": "08781351261207481557"
     },
     "user_tz": -540
    },
    "id": "bKainsNJSpHT",
    "outputId": "0f91d0dd-c731-4d9a-a3e7-3e0e0d5e2057"
   },
   "outputs": [],
   "source": [
    "#colab用にデータのpathを指定\n",
    "#My Driveのカンマはpath指定の時にはいらない\n",
    "\n",
    "DATA_PATH = \"/content/drive/My Drive/resarch\"\n",
    "!ls /content/drive/'My Drive'/resarch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HxtTlj3pObE8"
   },
   "outputs": [],
   "source": [
    "##データの読み込み\n",
    "import pickle\n",
    "\n",
    "## train data\n",
    "f=open(DATA_PATH+\"/x_train_corr_high.txt\",\"rb\")\n",
    "pre_data_train=pickle.load(f)\n",
    "f=open(DATA_PATH+\"/y_train_corr_high.txt\",\"rb\")\n",
    "label_train=pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tv_jnWeR4Kan"
   },
   "source": [
    "### 不均衡データなので、\n",
    "正解データと不例データから均等にサンプリングして複数データセットをひとつにまとめたものをデータセットにする<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RCIR4ExT4Kao"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "validationデータを正例、不例の一定割合データセットに分ける\n",
    "正例、不例のindexを取得しておく\n",
    "\n",
    "train時にはバッチサイズ64として、正例と不例を32ずつindexをサンプリングしたデータセットを複数作成して、まとめてデータセットとする\n",
    "正例、不例一定割合のデータセット作成\n",
    "'''\n",
    "\n",
    "buy=[]\n",
    "no_buy=[]\n",
    "for i in range(len(label_train)):\n",
    "    if label_train[i]==1:\n",
    "        buy.append(i)\n",
    "    else:\n",
    "        no_buy.append(i)\n",
    "\n",
    "x=[]\n",
    "y=[]\n",
    "\n",
    "'''make train data'''\n",
    "for i in range(2000):\n",
    "    posi=np.random.choice(buy,32,replace = False)\n",
    "    nega=np.random.choice(no_buy,32,replace = False)\n",
    "    for i in posi:\n",
    "        x.append(pre_data_train[i])\n",
    "        y.append(label_train[i])\n",
    "    for i in nega:\n",
    "        x.append(pre_data_train[i])\n",
    "        y.append(label_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8v-gFxGd4Kar"
   },
   "source": [
    "**hyperoptで調べたいこと<br>\n",
    "・convolutionの数、チャネルの数、filaの数、padの数<br>\n",
    "・最適化関数の設定**<br>\n",
    "※フィルタサイズとパディングは固定して、いくつか試して良い物を選択する。仮に複数フィルタサイズを試したいなら、for文を積むように書いて行う\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aC9GjgiM4Kar"
   },
   "outputs": [],
   "source": [
    "#ネットワーク定義\n",
    "\n",
    "out_chanel1=128\n",
    "out_chanel2=32\n",
    "out_chanel3=64\n",
    "k_size=3\n",
    "pad_size=1\n",
    "class Purchase(Chain):\n",
    "    def __init__(self,activate):\n",
    "        super(Purchase , self).__init__(\n",
    "            ##k_size:3\n",
    "            conv0=L.Convolution2D(1,out_chanel1,ksize=5,pad=1),\n",
    "            ##\n",
    "            conv1=L.Convolution2D(out_chanel1,out_chanel1,ksize=k_size,pad=pad_size),\n",
    "            conv2=L.Convolution2D(out_chanel1,out_chanel1,ksize=k_size,pad=pad_size),\n",
    "            conv3=L.Convolution2D(out_chanel1,out_chanel1,ksize=k_size,pad=pad_size),\n",
    "            ##\n",
    "            conv4=L.Convolution2D(out_chanel1,out_chanel1,ksize=k_size,pad=pad_size),\n",
    "            conv5=L.Convolution2D(out_chanel1,out_chanel1,ksize=k_size,pad=pad_size),\n",
    "            conv6=L.Convolution2D(out_chanel1,out_chanel1,ksize=k_size,pad=pad_size),\n",
    "            ##\n",
    "            conv7=L.Convolution2D(out_chanel1,out_chanel1,ksize=k_size,pad=pad_size),\n",
    "            conv8=L.Convolution2D(out_chanel1,out_chanel1,ksize=k_size,pad=pad_size),\n",
    "            conv9=L.Convolution2D(out_chanel1,out_chanel1,ksize=k_size,pad=pad_size),\n",
    "            ##\n",
    "            conv10=L.Convolution2D(out_chanel1,out_chanel1,ksize=k_size,pad=pad_size),\n",
    "            conv11=L.Convolution2D(out_chanel1,out_chanel1,ksize=k_size,pad=pad_size),\n",
    "            conv12=L.Convolution2D(out_chanel1,out_chanel1,ksize=k_size,pad=pad_size),\n",
    "            ##\n",
    "            conv13=L.Convolution2D(out_chanel1,out_chanel1,ksize=k_size,pad=pad_size),\n",
    "            conv14=L.Convolution2D(out_chanel1,out_chanel1,ksize=k_size,pad=pad_size),\n",
    "            conv15=L.Convolution2D(out_chanel1,out_chanel1,ksize=k_size,pad=pad_size),\n",
    "            ##\n",
    "            conv16=L.Convolution2D(out_chanel1,out_chanel1,ksize=k_size,pad=pad_size),\n",
    "            conv17=L.Convolution2D(out_chanel1,out_chanel1,ksize=k_size,pad=pad_size),\n",
    "            conv18=L.Convolution2D(out_chanel1,out_chanel1,ksize=k_size,pad=pad_size),\n",
    "            ##\n",
    "            conv19=L.Convolution2D(out_chanel1,out_chanel1,ksize=k_size,pad=pad_size),\n",
    "            conv20=L.Convolution2D(out_chanel1,out_chanel1,ksize=k_size,pad=pad_size),\n",
    "            conv21=L.Convolution2D(out_chanel1,out_chanel1,ksize=k_size,pad=pad_size),\n",
    "            ##\n",
    "            conv22=L.Convolution2D(out_chanel1,out_chanel1,ksize=k_size,pad=pad_size),\n",
    "            conv23=L.Convolution2D(out_chanel1,out_chanel1,ksize=k_size,pad=pad_size),\n",
    "            conv24=L.Convolution2D(out_chanel1,out_chanel1,ksize=k_size,pad=pad_size),\n",
    "            ##\n",
    "            conv25=L.Convolution2D(out_chanel1,out_chanel1,ksize=k_size,pad=pad_size),\n",
    "            conv26=L.Convolution2D(out_chanel1,out_chanel1,ksize=k_size,pad=pad_size),\n",
    "            conv27=L.Convolution2D(out_chanel1,out_chanel1,ksize=k_size,pad=pad_size),\n",
    "            ##\n",
    "            conv28=L.Convolution2D(out_chanel1,out_chanel1,ksize=k_size,pad=pad_size),\n",
    "\n",
    "                 \n",
    "            bn0=L.BatchNormalization(out_chanel1),\n",
    "            bn1=L.BatchNormalization(out_chanel1),\n",
    "            bn2=L.BatchNormalization(out_chanel1),\n",
    "            bn3=L.BatchNormalization(out_chanel1),\n",
    "            bn4=L.BatchNormalization(out_chanel1),\n",
    "            bn5=L.BatchNormalization(out_chanel1),\n",
    "            bn6=L.BatchNormalization(out_chanel1),\n",
    "            bn7=L.BatchNormalization(out_chanel1),\n",
    "            bn8=L.BatchNormalization(out_chanel1),\n",
    "            bn9=L.BatchNormalization(out_chanel1),\n",
    "            bn10=L.BatchNormalization(out_chanel1),\n",
    "            bn11=L.BatchNormalization(out_chanel1),\n",
    "            bn12=L.BatchNormalization(out_chanel1),\n",
    "            bn13=L.BatchNormalization(out_chanel1),\n",
    "            bn14=L.BatchNormalization(out_chanel1),\n",
    "            bn15=L.BatchNormalization(out_chanel1),\n",
    "            bn16=L.BatchNormalization(out_chanel1),\n",
    "            bn17=L.BatchNormalization(out_chanel1),\n",
    "            bn18=L.BatchNormalization(out_chanel1),\n",
    "            bn19=L.BatchNormalization(out_chanel1),\n",
    "            bn20=L.BatchNormalization(out_chanel1),\n",
    "            bn21=L.BatchNormalization(out_chanel1),\n",
    "            bn22=L.BatchNormalization(out_chanel1),\n",
    "            bn23=L.BatchNormalization(out_chanel1),\n",
    "            bn24=L.BatchNormalization(out_chanel1),\n",
    "            bn25=L.BatchNormalization(out_chanel1),\n",
    "            bn26=L.BatchNormalization(out_chanel1),\n",
    "            bn27=L.BatchNormalization(out_chanel1),\n",
    "            bn28=L.BatchNormalization(out_chanel1),\n",
    "\n",
    "            l1=L.Linear(None,2),\n",
    "           \n",
    "        \n",
    "        )\n",
    "        if activate == 'relu':\n",
    "            self.act = F.relu\n",
    "        elif activate == 'tanh':\n",
    "            self.act = F.tanh\n",
    "        elif activate == 'leakly_relu':\n",
    "            self.act = F.leaky_relu\n",
    "        \n",
    "    def __call__(self,x):\n",
    "            h0=self.act(self.bn0(self.conv0(x)))\n",
    "            h1=self.act(self.bn1(self.conv1(h0)))\n",
    "            h2=self.act(F.dropout(self.bn2(self.conv2(h1))))\n",
    "            h3=self.act(self.bn3(self.conv3(h2))+h1)\n",
    "            h4=self.act(F.dropout(self.bn4(self.conv4(h3))))\n",
    "            h5=self.act(self.bn5(self.conv5(h4))+h3)\n",
    "            h6=self.act(F.dropout(self.bn6(self.conv6(h5))))\n",
    "            h7=self.act(self.bn7(self.conv7(h6))+h5)\n",
    "            h8=self.act(F.dropout(self.bn8(self.conv8(h7))))\n",
    "            h9=self.act(self.bn9(self.conv9(h8))+h7)\n",
    "            h10=self.act(F.dropout(self.bn10(self.conv10(h9))))\n",
    "            h11=self.act(self.bn11(self.conv11(h10))+h9)\n",
    "            h12=self.act(F.dropout(self.bn12(self.conv12(h11))))\n",
    "            h13=self.act(self.bn13(self.conv13(h12))+h11)\n",
    "            h14=self.act(F.dropout(self.bn14(self.conv14(h13))))\n",
    "            h15=self.act(self.bn15(self.conv15(h14))+h13)\n",
    "            h16=self.act(F.dropout(self.bn16(self.conv16(h15))))\n",
    "            h17=self.act(self.bn17(self.conv17(h16))+h15)\n",
    "            h18=self.act(F.dropout(self.bn18(self.conv18(h17))))\n",
    "            h19=self.act(self.bn19(self.conv19(h18))+h17)\n",
    "            h20=self.act(F.dropout(self.bn20(self.conv20(h19))))\n",
    "            h21=self.act(self.bn21(self.conv21(h20))+h19)\n",
    "            h22=self.act(F.dropout(self.bn22(self.conv22(h21))))\n",
    "            h23=self.act(self.bn23(self.conv23(h22))+h21)\n",
    "            h24=self.act(F.dropout(self.bn24(self.conv24(h23))))\n",
    "            h25=self.act(self.bn25(self.conv25(h24))+h23)\n",
    "            h26=self.act(F.dropout(self.bn26(self.conv26(h25))))\n",
    "            h27=self.act(self.bn27(self.conv27(h26))+h25)\n",
    "            h28=self.act(F.dropout(self.bn28(self.conv28(h27))))\n",
    "\n",
    "            \n",
    "            return self.l1(F.dropout(h28))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pxUOlEqb4Ka9"
   },
   "outputs": [],
   "source": [
    "def main(params):\n",
    "    epoch = 20\n",
    "    gpu = 0\n",
    "    batchsize = 256\n",
    "\n",
    "    activate = params['activate']\n",
    "    optimizer_name = params['optimizer_name']\n",
    "    lr = params['lr']\n",
    "\n",
    "    model = L.Classifier(Purchase( activate))\n",
    "    \n",
    "    if gpu >= 0:\n",
    "        chainer.cuda.get_device(gpu).use()\n",
    "        model.to_gpu()\n",
    "\n",
    "    # Setup an optimizer\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = optimizers.Adam()\n",
    "    elif optimizer_name == 'AdaDelta':\n",
    "        optimizer = optimizers.AdaDelta()\n",
    "    else:\n",
    "        optimizer = optimizers.MomentumSGD(lr=lr)\n",
    "    optimizer.setup(model)\n",
    "    \n",
    "    ##load data\n",
    "    ##x,yのデータをtuple化して、更にvalidation dataを作成する\n",
    "\n",
    "    data= tuple_dataset.TupleDataset(x,y)\n",
    "    split_at=int(len(x)*0.95)\n",
    "    train,test=chainer.datasets.split_dataset(data ,split_at)\n",
    "    \n",
    "    ##trainer\n",
    "    train_iter = chainer.iterators.SerialIterator(train, batchsize)\n",
    "    test_iter = chainer.iterators.SerialIterator(test, batchsize,repeat=False, shuffle=False)\n",
    "\n",
    "    # Set up a trainer\n",
    "    updater = training.StandardUpdater(train_iter, optimizer, device=gpu)\n",
    "    trainer = training.Trainer(updater, (epoch, 'epoch'), out=\"hyperopt_result\")\n",
    "    \n",
    "    # Evaluate the model with the test dataset for each epoch\n",
    "    trainer.extend(extensions.Evaluator(test_iter, model, device=gpu))\n",
    "\n",
    "    # Write a log of evaluation statistics for each epoch\n",
    "    trainer.extend(extensions.LogReport())\n",
    "\n",
    "    # Save two plot images to the result dir\n",
    "    trainer.extend(\n",
    "        extensions.PlotReport(['main/loss', 'validation/main/loss'],\n",
    "                               'epoch', file_name='loss.png'))\n",
    "    trainer.extend(\n",
    "        extensions.PlotReport(\n",
    "            ['main/accuracy', 'validation/main/accuracy'],\n",
    "             'epoch', file_name='accuracy.png'))\n",
    "\n",
    "    # Write report\n",
    "    trainer.extend(extensions.PrintReport(\n",
    "        ['epoch', 'main/loss', 'validation/main/loss',\n",
    "         'main/accuracy', 'validation/main/accuracy', 'elapsed_time']))\n",
    "\n",
    "    # Print a progress bar to stdout\n",
    "    trainer.extend(extensions.ProgressBar())\n",
    "\n",
    "    # Run the training\n",
    "    trainer.run()\n",
    "    valid_data = trainer._extensions['PlotReport'].extension._data\n",
    "    loss_data = [data for i, data in valid_data['validation/main/loss']]\n",
    "    best10_loss = sorted(loss_data)[:10]\n",
    "    return sum(best10_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180183
    },
    "colab_type": "code",
    "id": "eJPWd6sO4Ka_",
    "outputId": "8f4163cf-0bf8-4b46-ba8f-741a86482b89",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    ##探索空間の設定\n",
    "    space = {\n",
    "        'activate':hp.choice('activate',('relu','tanh','leakly_relu')),\n",
    "        'optimizer_name':hp.choice('optimizer_name',('Adam','AdaDelta','MomentumSGD')),\n",
    "        'lr':hp.uniform('lr',0.005,0.02),\n",
    "        \n",
    "    }\n",
    "    best = fmin(main, space, algo=tpe.suggest, max_evals=20)\n",
    "    print(\"best parameters\",best)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "purchase_predict_learning_and_validation2_hyperopt_colab.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
